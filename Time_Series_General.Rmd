---
title: "Time Series Analysis - General"
author: "Dalya Adams"
date: "5/6/2018"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

###Books contains the daily sales of paperback and hardcover books at the same store. Forecast the next four days; sales for paperback and hardcover books

```{r}

library(fpp2)
data(books)
colnames(books)
```
### The seasonal trend in both hardcover and paperback sales is visible. There is a general upwward trend in hardcover sales, but not as present in paperback sales.
```{r}
plot(books, main="Daily Sales of Paperback and Hardcover", ylab="Daily Sales", xlab="Time")
```

##Use SES and explore different values for alpha. 

###Paperback Sales
```{r}
fit1<-ses(books[,1], alpha = 0.2, initial = "simple", h=1)
fit2<- ses(books[,1], alpha = 0.5, initial = "simple", h=1)
fit3<-ses(books[,1], alpha = 0.75, initial = "simple", h=1)
fit4<-ses(books[,1], alpha = 0.90, initial = "simple", h=1)

plot(books[,1], main="Daily Sales of Paperback", ylab="Daily Sales", xlab="Time")
lines(fitted(fit1), col="blue")
lines(fitted(fit2), col="red")
lines(fitted(fit3), col="purple")
lines(fitted(fit4), col="green")
legend("topleft", lty = 1, col = c(1,"blue", "red", "purple", "green"), 
       c("data", expression(alpha == 0.2), expression(alpha == 0.5), 
         expression(alpha == 0.75), expression(alpha == 0.9)), pch = 1, cex = 0.75)
```
### Hardcover Sales
```{r}
fit5<-ses(books[,2], alpha = 0.2, initial = "simple", h=1)
fit6<- ses(books[,2], alpha = 0.5, initial = "simple", h=1)
fit7<-ses(books[,2], alpha = 0.75, initial = "simple", h=1)
fit8<-ses(books[,2], alpha = 0.90, initial = "simple", h=1)

plot(books[,2], main="Daily Sales of Hardback", ylab="Daily Sales", xlab="Time")
lines(fitted(fit5), col="blue")
lines(fitted(fit6), col="red")
lines(fitted(fit7), col="purple")
lines(fitted(fit8), col="green")
legend("topleft", lty = 1, col = c(1,"blue", "red", "purple", "green"), 
       c("data", expression(alpha == 0.2), expression(alpha == 0.5), 
         expression(alpha == 0.75), expression(alpha == 0.9)), pch = 1, cex = 0.75)
```

## Record the within sample SSE for the one-step forecasts. Plot SSE against the alpha and find which value of alpha works best. 

###Paperback Sales
```{r}
fit1.sse<-sum(residuals(fit1)^2)
fit2.sse<-sum(residuals(fit2)^2)
fit3.sse<-sum(residuals(fit3)^2)
fit4.sse<-sum(residuals(fit4)^2)


sse<-c(fit1.sse, fit2.sse, fit3.sse, fit4.sse)
alpha<-c(0.2, 0.5, 0.75, 0.9)

plot(sse, alpha)
```
###Hardcover
```{r}
fit5.sse<-sum(residuals(fit5)^2)
fit6.sse<-sum(residuals(fit6)^2)
fit7.sse<-sum(residuals(fit7)^2)
fit8.sse<-sum(residuals(fit8)^2)


sse<-c(fit5.sse, fit6.sse, fit7.sse, fit8.sse)
alpha<-c(0.2, 0.5, 0.75, 0.9)

plot(sse, alpha)
```
###Paperback
```{r}
accuracy(fit1)
accuracy(fit2)
accuracy(fit3)
accuracy(fit4)
```

###Hardcover
```{r}
accuracy(fit5)
accuracy(fit6)
accuracy(fit7)
accuracy(fit8)
```
## What is the effect of alpha on the forecasts?

### The effect of alpha is different for the paperback sales versus the hardback sales. In reference to paperback sales, as the alpha increases, the SSE increases. This is shown in the graph, but also in the Mean Average Squared Error (MASE) presented in the summary plot. 

### In contrast, the hardcover sales has a smaller SSE and MASE at aplha equals 0.5. This is visible in the graph and theaccuracy stats. 

## Let SES select the optimal value of alpha. Use this value to generate forecasts for the next four days. 

###Paperback
```{r}

fit10<-ses(books[,1], initial= "simple", h=4)
plot(books[,1], xlim=c(0,35))
lines(fitted(fit10), col="blue")
lines(fit10$mean, col="green")

```

```{r}
summary(fit10)

```
```{r}
fit10.sse<-sum(residuals(fit10)^2)
fit10.sse
```

###Hardcover
```{r}

fit11<-ses(books[,2],initial= "simple", h=4)
plot(books[,2], xlim=c(0,35))
lines(fitted(fit11), col="red")
lines(fit11$mean, col="green")
```
```{r}
summary(fit11)
```

```{r}
fit11.sse<-sum(residuals(fit11)^2)
fit11.sse
```

###Lets see where these point lie in relation to the others

###Paperback

####As presented in the summary stats above, the alpha is 0.1685. This alpha value corresponds with the sse value of 33944.82, which is the lowest SSE value, or the most accurate forcast. 
```{r}
sse<-c(fit1.sse, fit2.sse, fit3.sse, fit4.sse, fit10.sse)
alpha<-c(0.2,  0.5, 0.75, 0.9, 0.2125)

plot(sse, alpha)
```

###Hardcover

#### As for Hardcover, the alpha value is selected as 0.3283 and the SSE is 30587.69, this combination has the lowest SSE, as shown in the graph below. This point is the one on the bottom left of the scatter plot. 
```{r}
sse<-c(fit5.sse, fit6.sse, fit7.sse, fit8.sse, fit11.sse)
alpha<-c(0.2,  0.5, 0.75, 0.9, 0.3473)

plot(sse, alpha)
```

##Repeat but with inital ="optimal". How much difference does an optimal initial level make?

###Paperback
```{r}
fit1.opt<-ses(books[,1],initial = "optimal", h=4)
plot(books[,1], xlim=c(0,35))
lines(fitted(fit1.opt), col="blue")
lines(fit1.opt$mean, col="green")
```
```{r}
summary(fit1.opt)
```
### Paperback SSE
```{r}
fit1.opt.sse<-sum(residuals(fit1.opt)^2)
fit1.opt.sse
```
## Identifying the optimal start point, decreases the SSEs. The SSE with the optimal start point and optimal alpha is plotted below. This point is located on the bottom left. It is obvious that this is the best model, in regards to SSE. 
```{r}
sse<-c(fit1.sse, fit2.sse, fit3.sse, fit4.sse, fit10.sse, fit1.opt.sse)
alpha<-c(0.2,  0.5, 0.75, 0.9, 0.2125, 0.1685)

plot(sse, alpha)
```

###Hardcover
```{r}
fit2.opt<-ses(books[,2],initial = "optimal", h=4)
plot(books[,2], xlim=c(0,35))
lines(fitted(fit2.opt), col="red")
lines(fit2.opt$mean, col="green")
```
```{r}
summary(fit2.opt)
```

###SSE
```{r}
fit2.opt.sse<-sum(residuals(fit2.opt)^2)
fit2.opt.sse
```

## The improvement in hardcover is not a dramatic as with paperback books. It is still an improvement, as shown in the scatterplot below. The optimal start point and alpha is the point on the bottom left, at alpha .32 with a SSE of 30587
```{r}
sse<-c(fit5.sse, fit6.sse, fit7.sse, fit8.sse, fit11.sse, fit2.opt.sse)
alpha<-c(0.2,  0.5, 0.75, 0.9, 0.3473, 0.3283)

plot(sse, alpha)
```


## Apply Holt's linear method to the paperback and hardback sereis and compute four-day forecasts in each case

##Paperback
```{r}
fit.7.2<-holt(books[,1], initial = "simple", h=4)

plot(books[,1], xlim=c(0,35))
lines(fitted(fit.7.2), col="red")
lines(fit.7.2$mean, col="blue")
```
```{r}
summary(fit.7.2)
```

##Hardback
```{r}
fit.7.2.2<-holt(books[,2], initial = "simple", h=4)

plot(books[,2], xlim=c(0,35))
lines(fitted(fit.7.2.2), col="red")
lines(fit.7.2.2$mean, col="blue")

```

```{r}
summary(fit.7.2.2)
```

## Compare the SSE measures of Holt's method for the two sereis to those of simple exponential smoothing in the previous questions. Discuss the merits of the two forecasting methods for these data sets. 

##Paperback
```{r}
fit.7.2.sse<-sum(residuals(fit.7.2)^2)
fit.7.2.sse


```
## All Paperback SSEs
```{r}
fit1.sse
fit2.sse
fit3.sse
fit4.sse
fit10.sse
fit1.opt.sse
```

##Hardback
```{r}
fit.7.2.2.sse<-sum(residuals(fit.7.2.2)^2)
fit.7.2.2.sse


```
#All Hardback SSEs
```{r}
fit5.sse
fit6.sse
fit7.sse
fit8.sse
fit11.sse
fit2.opt.sse
```

##Discuss the merits of the two forecasting methods for these data sets. 
### Judging by the SSEs, Holt's method does not perform better than the optimized SES model, with SSE values of 33944.82 for paperback and 30587.69 for hardback. The Holt values for these, respectively, are 46917.39 and 36842.1. 

## Compare the forecasts for the two series using both methods. Which do you think is best? 

###In looking at the graphs below, the holt forecast seems better, in that it captures the trend of the paperback and hardback sales. This is different from the conclusion drawn when viewing the SSE.

##Paperback
###Blue is Optimal SES forecast
###Red is Holts forecast
```{r}
plot(books[,1], xlim=c(0,35))

lines(fit1.opt$mean, col="blue")
lines(fit.7.2$mean, col="red")
```

#Hardback
###Blue is Optimal SES forecast
###Red is Holts forecast
```{r}
plot(books[,2], xlim=c(0,35))
lines(fit2.opt$mean, col="blue")
lines(fit.7.2.2$mean, col="red")
```

##Calculate the 95% prediction interval for the first forecast for each series using both methods, assuming normal errors. Compare your forecasts with those produced by R. 

##Paperback
```{r}
plot(fit.7.2)
```
```{r}
plot(fit1.opt)
```

##Hardcover
```{r}
plot(fit.7.2.2)
```

```{r}
plot(fit2.opt)
```

## Use data set eggs. Experiment with various options in the holt() function to see how much the forecasts change with damped or exponential trend. Also, try changin the parameter values for alpha and beta to see how the affect the forecasts

```{r}
data(eggs)

```


```{r}
holt1<-holt(eggs, alpha=0.8, beta=0.2, initial="simple", h=100)
holt2<-holt(eggs, alpha=0.8, beta=0.2, initial="simple", exponential=TRUE, h=100)
holt3<-holt(eggs, alpha=0.8, beta=0.2, damped=TRUE, initial="optimal", h=100)

```

```{r}
plot(eggs, xlim = c(1900,2100), ylim=c(0,350), main= "Holt Trend with Alpha=0.8 and Beta=0.2")
lines(holt1$mean, col="red")
lines(holt2$mean, col="blue")
lines(holt3$mean, col="purple")
legend("bottomleft", lty=1, col=c("black", "red", "blue", "purple"),
       c("Data", "Simple", "Exponential", "Damped"), cex = 0.75)
```

```{r}
holt4<-holt(eggs, alpha=0.2, beta=0.8, initial="simple", h=100)
holt5<-holt(eggs, alpha=0.2, beta=0.8, initial="simple", exponential=TRUE, h=100)
holt6<-holt(eggs, damped=TRUE, initial="optimal", h=100)

```

```{r}
plot(eggs, xlim = c(1900,2100), ylim=c(0,350),main= "Holt Trend with Alpha=0.2 and Beta=0.8")
lines(holt4$mean, col="red")
lines(holt5$mean, col="blue")
lines(holt6$mean, col="purple")
legend("bottomleft", lty=1, col=c("black", "red", "blue", "purple"),
       c("Data", "Simple", "Exponential", "Damped-Optimal"), cex = 0.75)
```

```{r}
holt7<-holt(eggs, h=100)
holt8<-holt(eggs, exponential=TRUE, h=100)
holt10<-holt(eggs, exponential=TRUE, damped=TRUE, h=100)
```

```{r}
plot(eggs, xlim = c(1900,2100), ylim=c(0,350), main= "Holt Trend with Optimal Alpha and Beta")
lines(holt7$mean, col="red")
lines(holt8$mean, col="blue")
lines(holt10$mean, col="purple")
legend("bottomleft", lty=1, col=c("black", "red", "blue", "purple"),
       c("Data", "R selected", "Exponential", "Damped"), cex = 0.75)
```
### Holt8 has the lowest RMSE. It is the model with exponential smoothing and R choosen the optimal alpha and beta values and no dampening. In the final graph, we can see that Holt8 is the blue line. It maintains the downward trend, but levels to a constant value. 
```{r}
accuracy(holt1)
accuracy(holt2)
accuracy(holt3)
accuracy(holt4)
accuracy(holt5)
accuracy(holt6)
accuracy(holt7)
accuracy(holt8)
accuracy(holt10)
```

##Use data set ukcars. Plot the data and describe the main features of the series

#### There is a strong seasonal effect in this dataset, and an inconsistent trend. In the beginning of the dataset, we see a downward trend, but it becomes an upward trend which levels out towards the end of the dataset. The most prevalent feature is the seasonal variation present in the data. 
```{r}
data(ukcars)
plot(ukcars, main="UK Passenger Vehicle Production Data")
```

## Decompose the series using STL and obtain the seasonally adjusted data
```{r}
seas<-stl(ukcars, s.window = "periodic")
plot(seas)
seas.adj<-seasadj(seas)
```

## Forecast the next 2 years using an additive dampened trend method applied to seasonally adjusted data. Then reseasonalize the forecasts. Record the parameters of the methods and report the RMSE of the one-step forecasts from your method. 

```{r}
fcast<-holt(seas.adj, damped = TRUE, seasonal="additive", h=24, robust=TRUE)
#View(seas.adj)
accuracy(fcast)
fcast$model
```
```{r}
plot(forecast(fcast, h=24))

```

```{r}
fcast2 <- forecast(seas, method="naive")
plot(fcast2)
```

##Forecast the next 2 years using Holt's linear method applied to seasonally adjusted data. Then reseasonalize the forecasts. Record the parameters of the methods and report the RMSE of the one-step forecasts from your method. 

```{r}
fcast3<-holt(seas.adj, initial = "simple", h=24, robust=TRUE)
#View(seas.adj)
accuracy(fcast3)
fcast3$model
```

```{r}
linpred<-forecast(fcast3, h=24)
plot(forecast(fcast3, h=24))
```

```{r}
fcast4 <- forecast(seas, method= "naive", h=24)
plot(fcast4)

```
```{r}
summary(fcast4)
```

##Now use the ets() to choose a seasonal model for the data. 
```{r}
fets<- ets(ukcars)
summary(fets)

```

```{r}
plot(forecast(fets, h=24))
```

##Compare the RMSE of the fitted model with the RMSE of the model you obtained using STL Decomposition with Holt's Methos. Which gives the better in-sample fits?

###The RMSE for the Holt method on the seasonally adjusted data is better than the naive forecast on the seasonal data. The ETS model selected by the ets() fit has a competitive RMSE to the Holt method on the seasonally adjsuted data. 

```{r}
accuracy(fets)
accuracy(fcast)
accuracy(fcast4)
```

##Compare the forecasts from the two approaches? Which seems most reasonable? 

###In viewing the forecasted results, the ETS model has a smaller prediction interval and is slightly more accuracte than the reseasonalized prediction (fcast4) in all accuracy measures, except ME. 

## Use R to simulate and plot some data from simple ARIMA models

## Generate data from an AR(1) model with theta=0.6 and sigma squared = 1. Start with y0=0

###theta = ar ,Y0= c, sqrt(sigma^2) = sd

```{r}
sim1<-arima.sim(list( ar=0.6, sd=sqrt(1), c=0), n=100)
```

## Produce a time plot for the series. How does the plot change as you change theta?
```{r}
plot(sim1)
```
```{r}
sim1<-arima.sim(list( ar=0.6, sd=sqrt(1), c=0), n=100)
sim1.1 <-arima.sim(list( ar=.45, sd=sqrt(1), c=0), n=100)
sim1.2 <-arima.sim(list( ar=0.2, sd=sqrt(1), c=0), n=100)
sim1.3 <-arima.sim(list( ar=0, sd=sqrt(1), c=0), n=100)
sim1.4 <-arima.sim(list( ar=.80, sd=sqrt(1), c=0), n=100)
```
###Well, that looks like abstract art
```{r}
plot(sim1)
lines(sim1.1, col="red")
lines(sim1.2, col= "blue")
lines(sim1.3, col="purple")
lines(sim1.4, col= "green")
legend("bottomleft", lty = 1, col = c(1, "red","blue", "purple", "green"), 
       c(expression(AR==0.6), expression(AR==0.45), expression(AR==0.20), 
         expression(AR==0.0), expression(AR==0.80)), pch = 1, cex = 0.50)
```
```{r}
plot(sim1)
lines(sim1.1, col="red")
legend("bottomleft", lty = 1, col = c(1, "red"), c(expression(AR==0.6), 
                                                   expression(AR==0.45)), 
       pch = 1, cex = 0.70)
```

```{r}
plot(sim1)
lines(sim1.2, col= "blue")
legend("bottomleft", lty = 1, col = c(1, "blue"), 
       c(expression(AR==0.6),  expression(AR==0.20)), pch = 1, cex = 0.70)
```

```{r}
plot(sim1)
lines(sim1.3, col="purple")
legend("bottomleft", lty = 1, col = c(1, "purple"), 
       c(expression(AR==0.6),  expression(AR==0.0)), pch = 1, cex = 0.70)
```

```{r}
plot(sim1)
lines(sim1.4, col= "green")
legend("bottomleft", lty = 1, col = c(1,"green"), 
       c(expression(AR==0.6),  expression(AR==0.80)), pch = 1, cex = 0.70)
```


##Generate data from an MA(1) model with theta = 0.6, and sigma squared = 1, start with e0=0. 
```{r}
sim2<-arima.sim(list( ma=0.6, sd=sqrt(1), c=0), n=100)
```

##Produce a time plot for the sereis. How does the plot change as you change the theta

```{r}
plot(sim2)
```

```{r}
sim2<-arima.sim(list( ma=0.6, sd=sqrt(1), c=0), n=100)
sim2.1<-arima.sim(list( ma=-1.0, sd=sqrt(1), c=0), n=100)
sim2.2<-arima.sim(list( ma=1.0, sd=sqrt(1), c=0), n=100)
sim2.3<-arima.sim(list( ma=-0.50, sd=sqrt(1), c=0), n=100)
sim2.4<-arima.sim(list( ma=0.50, sd=sqrt(1), c=0), n=100)
```

```{r}
plot(sim2)
lines(sim2.1, col="red")
lines(sim2.2, col= "blue")
lines(sim2.3, col="purple")
lines(sim2.4, col= "green")
legend("bottomleft", lty = 1, col = c(1, "red","blue", "purple", "green"), 
       c(expression(MA==0.6), expression(MA==-1.0), 
         expression(MA==1.00), expression(MA==-0.50), expression(MA==0.50)), 
       pch = 1, cex = 0.50)
```

```{r}
plot(sim2)
lines(sim2.1, col="red")
legend("bottomleft", lty = 1, col = c(1, "red"), 
       c(expression(MA==0.6), expression(MA==-1.0)), 
       pch = 1, cex = 0.50)
```

```{r}
plot(sim2)
lines(sim2.2, col= "blue")
legend("bottomleft", lty = 1, col = c(1, "blue"), 
       c(expression(MA==0.6),  expression(MA==1.00)), pch = 1, cex = 0.50)
```

```{r}
plot(sim2)
lines(sim2.3, col="purple")
legend("bottomleft", lty = 1, col = c(1,  "purple"), 
       c(expression(MA==0.6),  expression(MA==-0.50)), pch = 1, cex = 0.50)
```

```{r}
plot(sim2)
lines(sim2.4, col= "green")
legend("bottomleft", lty = 1, col = c(1 ,"green"), 
       c(expression(MA==0.6),  expression(MA==0.50)), pch = 1, cex = 0.50)
```

##Generate data from an ARMA (1,1) model with AR=0.6 and MA=0.6 and sd=sqrt(1). Start with y0=y-1=0
```{r}
sim3<-arima.sim(list( ar=0.6, ma=0.6, sd=sqrt(1), c=0), n=100)
```


##Generate data from an AR(2) model with AR=-0.8 and 0.3 and sd=sqrt(1). STart with y0=y-1=0
```{r}
sim4<-arima.sim(list( ar=c( 0.3,-0.8), sd=sqrt(1), c=0), n=100)
```

##Graph the latter two series and compare them. 

###The ARMA with values (-.8,.3) primarily stays centered around 0. It does show ebbs and flows, but is lacking the peaks seen in the ARMA with values (.6,.6). With this ARMA model, the model is centered around 0, but with extreme peaks and dips. TO stay centered around 0, these extreme peaks are followed by extreme dips. 
```{r}
plot(sim3)
lines(sim4, col="red")
legend("bottomleft", lty = 1, col = c(1 ,"red"), 
       c(expression(AR/MA==0.6),  expression(AR==0.3/-0.8)), pch = 1, cex = 0.70)
```

##Data set wmurders
```{r}
data(wmurders)
```

##By studying appropriate graphs of the series in R, find an appropriate ARIMA(p,d,q) model for these data

###In looking at the plot, we first notice the model is not stationary. We'll take the first difference to try and make it stationary. 
```{r}
plot(wmurders)
```
```{r}
par(mfrow=c(1,2))
acf(wmurders)
pacf(wmurders)
```

```{r}
plot(diff(wmurders, 1))
```

```{r}
par(mfrow=c(1,2))
acf(diff(wmurders, 1))
pacf(diff(wmurders, 1))
```

```{r}
plot(diff(diff(wmurders, 1),2))
```
##AR
```{r}
par(mfrow=c(1,2))
acf(diff(diff(wmurders, 1),2))
pacf(diff(diff(wmurders, 1),2))
```
##Yeah! I got it right!!
```{r}
ndiffs(wmurders)
```
##Now lets try and build this model

```{r}
dif2<-diff(diff(wmurders, 1),2)
```


ARIMA(1,2,2)?
```{r}
par(mfrow=c(1,2))
acf(dif2)
pacf(dif2)
```

```{r}
fitA1<-Arima(wmurders, order = c(1,2,3))
summary(fitA1)
```

```{r}
fitA<-Arima(wmurders, order = c(2,2,3))
summary(fitA)
```

##Should you include a constant in the model? Explain. 
### The model that i've selected above has a d>1, so adding a constant is particularly dangerous when forecasting. 


## Fit the model using R and examine the residuals. Is the model satisfactory? 

###The model appears to be satisfactory. There are no significant spikes in the ACF or PACF and the residuals appear to be white noise. 
```{r}
tsdisplay(residuals(fitA))
```

##Forecast three times ahead. 
```{r}

forecast(fitA, h=3)
```


## Create a plot of the series with forecasts and prediction intervals for the next three periods shown. 
```{r}
plot(forecast(fitA, h=3))
```

##Does auto.arima give the same model you have chosen? If not, which model do you think is better? 

###The Auto.Arima model did not choose the same model. If we base our selected on the smallest AICc, we choose the auto.arima model. If we go based off of the RMSE, we choose the ARIMA(2,2,3). Since the models are different, we will use the RMSE and go with the handcrafted ARIMA(2,2,3) model. 
```{r}
aafit<-auto.arima(wmurders)
summary(aafit)
```

## Find the latest data and compare with your forecasts. 
```{r}
forecast(fitA, h=3)

```
### The latest data isn't present on the website. 

##Dataset austourists
```{r}
data("austourists")
```

##Describe the time plot. 
###This dataset has strong seasonal trend, and a general upward trend.
```{r}
plot(austourists)
```


###The ACF Plot shows the relationship between yt and y(t-k). So this ACF shows us that the value of yt is impacted by previous values, specifically, 1 year prior. The seasonal impact is likely yearly in this quarterly dataset.  We also can learn that the dataset is not stationary, which we noticed in the plot of the data. 
```{r}
acf(austourists)
```


###The PACF shows us the relationship of yt to y(t-k) after removing the effects of other time lags prior to y(t-k). We notice the same yearly spikes in the PACF and the non stationary aspect of the dataset. 
```{r}
pacf(austourists)
```

## Produce plots of the seasonally differenced data (1-B^4)Yt. 
```{r}
#seasd<-seasadj(stl(austourists, s.window = "periodic"))
tsdisplay(diff(austourists, lag = 4, differences = 1))
```
##What model do these graphs suggest? 
### The significant lag at 1 in the ACF suggests a non-seasonal MA(1), the significant spike at 4 in the ACF suggests a seasonal MA(1) component. So we being with a ARIMA (0,1,1)(0,1,1)[4] model. The PACF shows a significant spike at 1,4 and 5. The spike at 4 is consistent with teh seasonal nature of the data. We end with a ARIMA (1,1,1)(1,1,1)[4] model. 

###The auto arima model chose a different model than the one I selected. In running the model I selected, the model selected by the auto.arima is better, as measured by the smaller AICc and smaller RMSE
```{r}
aa8.7<-auto.arima(austourists)
summary(aa8.7)
```
```{r}
aamy<-Arima(austourists, order = c(1,1,1), seasonal = c(1,1,1))
summary(aamy)
```

##Dataset usmelec. In general there are two peaks per year: mid-summer and mid-winter. 
```{r}
data("usmelec")
```

## Examine the 12 month moving average of this series to see what kind of trend is involved. 
```{r}
plot(usmelec)
```

```{r}
ma12<-ma(usmelec, 12)
plot(usmelec)
lines(ma12, col="blue")
```

##The seasonal effect of the data increases as time goes on. This dataset will likely benefit from a log transofrmaiton, which will remove the increased varaince of the later observations. 

```{r}
lusmelec<-log(usmelec)
lma12<-ma(lusmelec, 12)
plot(lusmelec)
lines(lma12, col="blue")
```

###The data is not stationary, but a single difference appears to make the dataset stationary. 

```{r}
tsdisplay(diff(lusmelec, lag = 12, differences = 1))
```
```{r}
tsdisplay(diff(diff(lusmelec, lag = 12, differences = 1)))
```

### In looking at the ACF and PACF, we see exponential decay in the ACF, with a significant spike at 12. In the PACF, There is a significant lag at 1,3, 11,12,13,23,24,25,36. 
###PACF is AR -- ARIMA(AR,D,MA)
###ARIMA(2,0,0)(3,1,1)[12] is the best model of the ones tested below, based off of the AIC, AICc and BIC. 

```{r}
arima1<-Arima(lusmelec, order = c(2,0,0), seasonal = c(3,1,1))
summary(arima1)
```

```{r}
arima2<-Arima(lusmelec, order = c(2,1,3), seasonal = c(3,1,1))
summary(arima2)
```

```{r}
arima2<-Arima(lusmelec, order = c(2,1,2), seasonal = c(3,1,1))
summary(arima2)
```

## Estimate the parameters of your best model and do diagnostic testing on the residuals. Do the residuals resemble white noise? If not, try to find another ARIMA model which fits better. 

```{r}
summary(arima1)
```
### The ACF and PACF show that there is still correlation left in the residuals. A better model will be estimated. The ACF and PACF both show significant spikes at 2 and 10. 
```{r}
tsdisplay(residuals(arima1))
```
##P-values are extremely small, so the residuals are distinguishable from white noise. Prediciton can be used by not the forecast intervals. 
```{r}
Box.test(residuals(arima1), lag = 12, fitdf = 6, type = "Ljung")
```
####ARIMA (3,0,0)(1,1,1)[12]
```{r}
arima4<-Arima(lusmelec, order = c(3,0,0), seasonal = c(1,1,1))
summary(arima4)
```

####ARIMA (2,0,1)(1,1,1)[12]
```{r}
arima5<-Arima(lusmelec, order = c(2,0,1), seasonal = c(3,1,1))
summary(arima5)
```

```{r}
arima7<-Arima(lusmelec, order = c(2,0,1), seasonal = c(3,1,1))
summary(arima7)
```
# significant spikes at 9 and 15, missing some non seasonal 
```{r}
tsdisplay(residuals(arima7))
```


```{r}
arima8<-Arima(lusmelec, order = c(3,0,2), seasonal = c(3,1,1))
summary(arima8)
```

```{r}
tsdisplay(residuals(arima8))
```

```{r}
arima9<-Arima(lusmelec, order = c(2,1,3), seasonal = c(3,1,1))
summary(arima9)
```

```{r}
tsdisplay(residuals(arima9))
```
###What does the auto arima say it should be?
```{r}
aa<-auto.arima(usmelec)
summary(aa)
```

```{r}
tsdisplay(residuals(aa))
```

##Forecast the next 15 years. 
```{r}
plot(forecast(arima7), n=180)
```

###The further from the last observation we get, the less accurate and reliable the forecast is likely to be. Since we are forecasting 15 years into the future, the forecasts from year 10 on, may be highly suspect. Another issue to be wary about is the prediction intervals. Since I was unable to account for all significant spikes in building a seasonal ARIMA model, the prediction intervals should not be relied upon. The forecasts can be relied upon. 


